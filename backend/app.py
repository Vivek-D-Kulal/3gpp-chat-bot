from flask import Flask, request, jsonify
from flask_cors import CORS
from sentence_transformers import SentenceTransformer, util
from collections import OrderedDict

import numpy as np
import os
import networkx as nx
import pickle
import json
import torch
import time
from openai import OpenAI

# Load environment key
from dotenv import load_dotenv
load_dotenv()

app = Flask(__name__)
CORS(app)

client = OpenAI()

CACHE_PATH = os.path.join(os.path.dirname(__file__), "cache_gpt_responses.json")

def load_cache():
    if os.path.exists(CACHE_PATH):
        with open(CACHE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            return OrderedDict(data)
    return OrderedDict()

MAX_CACHE_SIZE = 50  # You can change this anytime

def save_cache(query, answer):
    cache = load_cache()

    # Move existing query to the end if it exists
    if query in cache:
        del cache[query]

    # Add new entry
    cache[query] = answer

    # Trim to max size
    while len(cache) > MAX_CACHE_SIZE:
        cache.popitem(last=False)  # Removes the oldest entry

    with open(CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=2)

def find_similar_cached_response(query_text, threshold=0.85):
    cache = load_cache()
    if not cache:
        return None

    cache_queries = list(cache.keys())
    query_embedding = model.encode(query_text, convert_to_tensor=True)
    cache_embeddings = model.encode(cache_queries, convert_to_tensor=True)

    similarities = util.pytorch_cos_sim(query_embedding, cache_embeddings)[0]
    best_idx = int(np.argmax(similarities.cpu().numpy()))
    best_score = similarities[best_idx].item()

    if best_score >= threshold:
        print(f"ü§ù Fuzzy matched with: '{cache_queries[best_idx]}' (score={best_score:.2f})")
        return cache[cache_queries[best_idx]]

    return None

def detect_answer_length(query: str) -> str:
    query = query.lower()
    if any(kw in query for kw in ["in short", "briefly", "summary", "quickly", "short answer"]):
        return "short"
    elif any(kw in query for kw in ["in detail", "elaborate", "long answer", "full explanation", "explain thoroughly"]):
        return "long"
    else:
        return "normal"

# Load graph
with open(os.path.join(os.path.dirname(__file__), "../data/unified_graph.pkl"), "rb") as f:
    G = pickle.load(f)

# Load model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Build documents
node_ids = []
corpus = []

for node_id, attr in G.nodes(data=True):
    text = attr.get("text") or ""
    title = attr.get("title") or ""
    if not isinstance(text, str):
        text = str(text)
    if not isinstance(title, str):
        title = str(title)

    combined = f"{title}. {text}".strip()
    if combined:
        corpus.append(combined)
        node_ids.append(node_id)

corpus_embeddings = model.encode(corpus, convert_to_tensor=True)

@app.route("/api/query", methods=["POST"])
def query():
    try:
        start_time = time.perf_counter()
        data = request.get_json(force=True)
        print("Received data from frontend:", data)

        query_text = data.get("query", "").strip().lower()
        if not query_text:
            return jsonify({"answer": "Please enter a valid question.", "highlight": []})

        # Encode query and compute similarity
        query_embedding = model.encode(query_text, convert_to_tensor=True)

        # Compute cosine similarities
        similarities = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
        top_idx = int(np.argmax(similarities.cpu().numpy()))
        top_node_id = node_ids[top_idx]
        top_data = G.nodes[top_node_id]
        highlights = [top_node_id]

        # Prepare context: top node + limited neighbors, truncating text
        def format_section(nid):
            data = G.nodes.get(nid, {})
            title = data.get("title", nid)
            text = data.get("text", "")
            return f"{title}:\n{text[:500]}..."  # truncate long texts

        context_sections = [format_section(top_node_id)]
        neighbors = top_data.get("neighbors", {})

        neighbor_limit = 4  # You can increase/decrease this as needed
        neighbor_count = 0

        for rel in ("parent", "siblings", "children"):
            val = neighbors.get(rel)
            if isinstance(val, list):
                for nid in val:
                    if neighbor_count >= neighbor_limit:
                        break
                    context_sections.append(format_section(nid))
                    highlights.append(nid)
                    neighbor_count += 1

        full_context = "\n\n".join(context_sections)
        
        # üîç Try fuzzy match from cache
        cached_answer = find_similar_cached_response(query_text)
        if cached_answer:
            end_time = time.perf_counter()
            print(f"‚ö° Served from fuzzy cache. Done in {end_time - start_time:.2f}")
            return jsonify({
                "answer": cached_answer,
                "highlight": [],
            })

        # Construct AI prompt
        prompt = f"""
You are an expert assistant helping explain telecom technical documentation to engineers and curious professionals.

Strictly use only the information from the context below to answer the user's question. 
Do not add external knowledge or make up content.

Format your answer in a clean and structured way:
- Use plain text for explanations.
- Use bullet points or tables only when the content naturally fits that format (e.g., lists of features, differences, conditions).
- Avoid redundant phrases or overuse of formatting.

---

Question:
{query_text}

---

Context:
{full_context}
"""

        length_pref = detect_answer_length(query_text)

        if length_pref == "short":
            max_tokens = 150
            style_note = "Respond briefly and to the point."
        elif length_pref == "long":
            max_tokens = 800
            style_note = "Provide a detailed explanation with examples if needed."
        else:
            max_tokens = 400
            style_note = "Keep your response concise but informative."

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": f"You are a helpful technical assistant. Only answer based on the provided context. Structure your response naturally using plain text, bullet points, or tables when appropriate. {style_note}"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=max_tokens
            )
            print("Full context:", full_context)
            answer = response.choices[0].message.content.strip()
            end_time = time.perf_counter()
            print(f"‚ö° Generated the answer searching therough the nodes in {end_time - start_time:.2f}")
        except Exception as e:
            print("‚ùå Error during OpenAI request:", e)
            return jsonify({"answer": "Sorry, the AI model failed to respond.", "highlight": []}), 500
        
        print("üîç Final Answer Preview:\n", answer)
        save_cache(query_text, answer)

        return jsonify({
            "answer": answer,
            "highlight": highlights,
        })

    except Exception as e:
        print("Error processing query:", e)
        return jsonify({"answer": "Server error occurred.", "highlight": []}), 500

# @app.route('/api/graph', methods=['GET'])
# def get_graph():
#     filepath = os.path.join(os.path.dirname(__file__), '..', 'data', 'unified_graph.pkl')
#     filepath = os.path.abspath(filepath)

#     with open(filepath, "rb") as f:
#         data = pickle.load(f)

#     graph_data = {
#         "nodes": [{"id": str(node)} for node in data.nodes()],
#         "links": [{"source": str(u), "target": str(v)} for u, v in data.edges()]
#     }
#     return jsonify(graph_data)

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=5000)